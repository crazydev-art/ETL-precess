tmpfile    = "temp.tmp"               # file used to store all extracted data
logfile    = "logfile.txt"            # all event logs will be stored in this file
targetfile = "transformed_data.csv"   # file where transformed data is stored
coulumn = ['name', 'height', 'weight']

pathnam_csv = "/Users/yassineoc/Desktop/VisualStudio/Coursera/source/*.csv"
pathnam_json = "/Users/yassineoc/Desktop/VisualStudio/Coursera/source/*.json"
pathnam_xml= "/Users/yassineoc/Desktop/VisualStudio/Coursera/source/*.xml"


# function to extract data from csv files  
def extract_from_csv(file_to_process):
    dataframe = pd.read_csv(file_to_process)
    return dataframe

# function to extract data from json files  
def extract_from_json(file_to_process):
    dataframe = pd.read_json(file_to_process,lines=True)
    return dataframe


# function to extract data from xml files  
def extract_from_xml(file_to_process):
    data_list = []
    tree = ET.parse(file_to_process)
    root = tree.getroot()
    
    for person in root:
        name = person.find("name").text
        height = float(person.find("height").text)
        weight = float(person.find("weight").text)
        
        data_list.append({"name": name, "height": height, "weight": weight})
    
    dataframe = pd.DataFrame(data_list)
    return dataframe


# function for extracting all files

def extract():
    extracted_data = pd.DataFrame(columns=['name','height','weight']) # create an empty data frame to hold extracted data
    
    # function for extracting all csv files
    for file in glob.glob(pathnam_csv):
        extracted_data = pd.concat([extracted_data,extract_from_csv(file)], ignore_index=True)

    # function for extracting all json files
    for file in glob.glob(pathnam_json):
        extracted_data = pd.concat([extracted_data,extract_from_json(file)], ignore_index=True)

    # function for extracting all xml files
    for file in glob.glob(pathnam_xml):
        extracted_data = pd.concat([extracted_data,extract_from_xml(file)], ignore_index=True)
    
    return extracted_data


#-------------------transorming data------------------------

def transform(data):
    #transform data height from inches to milimiters
    data["height"] = round(data.height*0.0254,2)
    #transform data weight from pound to kilogram
    data["weight"] = round(data.weight*0.45359237,2)

    return data 


#--------------loading data-------------------------------
def load(targetfile,data_to_load):
    data_to_load.to_csv(targetfile)  


#---------------creating log files--------------

def log(message):
    timestamp_format = '%Y-%h-%d-%H:%M:%S' # Year-Monthname-Day-Hour-Minute-Second
    now = datetime.now() # get current timestamp
    timestamp = now.strftime(timestamp_format)
    with open("logfile.txt","a") as f:
        f.write(timestamp + ',' + message + '\n')


#----------------- ETL process---------------
log("ETL Job Started")

log("Extract phase Started")
extracted_data = extract()
log("Extract phase Ended")
print(extracted_data)

log("Transform phase Started")
transformed_data = transform(extracted_data)
log("Transform phase Ended")
print(transformed_data) 


log("Load phase Started")
load(targetfile,transformed_data)
log("Load phase Ended")

log("ETL Job Ended")
